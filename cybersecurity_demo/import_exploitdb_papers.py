"""
Import Exploit-DB Papers into Vector Store
‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ security papers ‡∏à‡∏≤‡∏Å Exploit-DB ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà‡∏£‡∏∞‡∏ö‡∏ö RAG
"""

import os
import sys
import csv
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from datetime import datetime
from tqdm import tqdm
import hashlib

class ExploitDBImporter:
    def __init__(self, 
                 papers_path="../exploitdb-papers-main",
                 persist_directory="./chroma_db_exploitdb"):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Importer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Exploit-DB Papers
        
        Args:
            papers_path: path ‡πÑ‡∏õ‡∏¢‡∏±‡∏á exploitdb-papers-main folder
            persist_directory: ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö Vector Database
        """
        self.papers_path = papers_path
        self.persist_directory = persist_directory
        
        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ papers_path ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡∏à‡∏£‡∏¥‡∏á
        if not os.path.exists(papers_path):
            raise FileNotFoundError(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö folder: {papers_path}")
        
        print(f"üìÅ Papers path: {papers_path}")
        print(f"üíæ Vector DB path: {persist_directory}")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á directory ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö vector store
        os.makedirs(persist_directory, exist_ok=True)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á ChromaDB client
        print("üîß Initializing ChromaDB...")
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á collection ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö exploit papers
        # ‡∏•‡∏ö collection ‡πÄ‡∏Å‡πà‡∏≤‡∏ñ‡πâ‡∏≤‡∏°‡∏µ
        try:
            old_collection = self.client.get_collection("exploitdb_papers")
            old_count = old_collection.count()
            if old_count > 0:
                print(f"‚ö†Ô∏è  Found existing collection with {old_count} documents")
                print("üóëÔ∏è  Deleting old collection...")
                self.client.delete_collection("exploitdb_papers")
                print("‚úÖ Old collection deleted")
        except:
            pass
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á collection ‡πÉ‡∏´‡∏°‡πà
        self.collection = self.client.create_collection(
            name="exploitdb_papers",
            metadata={"description": "Exploit-DB Security Papers"}
        )
        print("‚úÖ Created new collection: exploitdb_papers")
        
        # ‡πÇ‡∏´‡∏•‡∏î Embedding Model
        print("üì• Loading embedding model (all-MiniLM-L6-v2)...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("‚úÖ Embedding model loaded")
    
    def load_papers_metadata(self):
        """‡πÇ‡∏´‡∏•‡∏î metadata ‡∏à‡∏≤‡∏Å files_papers.csv"""
        csv_path = os.path.join(self.papers_path, "files_papers.csv")
        
        if not os.path.exists(csv_path):
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏ü‡∏•‡πå: {csv_path}")
            return []
        
        papers = []
        with open(csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                papers.append(row)
        
        print(f"üìä Found {len(papers)} papers in metadata")
        return papers
    
    def read_paper_content(self, paper_path):
        """‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå paper"""
        full_path = os.path.join(self.papers_path, paper_path)
        
        if not os.path.exists(full_path):
            return None
        
        try:
            # ‡∏•‡∏≠‡∏á‡∏≠‡πà‡∏≤‡∏ô‡∏î‡πâ‡∏ß‡∏¢ encoding ‡∏ï‡πà‡∏≤‡∏á‡πÜ
            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
            
            for encoding in encodings:
                try:
                    with open(full_path, 'r', encoding=encoding) as f:
                        content = f.read()
                        # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏à‡∏£‡∏¥‡∏á
                        if content and len(content) > 50:
                            return content
                except:
                    continue
            
            return None
        except Exception as e:
            print(f"‚ö†Ô∏è  Error reading {paper_path}: {str(e)}")
            return None
    
    def chunk_text(self, text, chunk_size=800, overlap=100):
        """‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô chunks (‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)"""
        if not text or len(text) < chunk_size:
            return [text] if text else []
        
        # ‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î
        max_chunks = 50
        
        chunks = []
        start = 0
        
        while start < len(text) and len(chunks) < max_chunks:
            end = start + chunk_size
            
            # ‡∏´‡∏≤ newline ‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥
            if end < len(text):
                newline_pos = text.rfind('\n', start, end)
                if newline_pos > start:
                    end = newline_pos
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            start = end - overlap if end < len(text) else end
        
        return chunks
    
    def import_papers(self, language='english', limit=None):
        """
        ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ papers ‡πÄ‡∏Ç‡πâ‡∏≤‡∏™‡∏π‡πà vector store
        
        Args:
            language: ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ import (default: english)
            limit: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞ import (None = ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
        """
        print(f"\nüöÄ Starting import process...")
        print(f"   Language: {language}")
        print(f"   Limit: {limit if limit else 'All'}")
        print("=" * 60)
        
        # ‡πÇ‡∏´‡∏•‡∏î metadata
        all_papers = self.load_papers_metadata()
        
        # ‡∏Å‡∏£‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏†‡∏≤‡∏©‡∏≤
        papers = [p for p in all_papers if p.get('language') == language]
        
        if limit:
            papers = papers[:limit]
        
        print(f"üìù Will process {len(papers)} papers\n")
        
        imported_count = 0
        error_count = 0
        
        for paper in tqdm(papers, desc="Importing papers"):
            try:
                paper_id = paper.get('id', '')
                paper_file = paper.get('file', '')
                description = paper.get('description', '')
                author = paper.get('author', '')
                date_published = paper.get('date_published', '')
                tags = paper.get('tags', '')
                
                # ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                content = self.read_paper_content(paper_file)
                
                if not content:
                    error_count += 1
                    continue
                
                # ‡∏Ç‡πâ‡∏≤‡∏° paper ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (> 500KB)
                if len(content) > 500000:
                    error_count += 1
                    continue
                
                # ‡πÅ‡∏ö‡πà‡∏á content ‡πÄ‡∏õ‡πá‡∏ô chunks
                chunks = self.chunk_text(content)
                
                if not chunks:
                    error_count += 1
                    continue
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings ‡πÅ‡∏•‡∏∞‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á ChromaDB
                doc_id = f"paper_{paper_id}"
                for i, chunk in enumerate(chunks):
                    chunk_id = f"{doc_id}_chunk_{i}"
                    
                    # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding
                    embedding = self.embedding_model.encode(chunk).tolist()
                    
                    # ‡πÄ‡∏Å‡πá‡∏ö‡∏•‡∏á collection
                    self.collection.add(
                        ids=[chunk_id],
                        embeddings=[embedding],
                        documents=[chunk],
                        metadatas=[{
                            'paper_id': paper_id,
                            'title': description,
                            'author': author,
                            'date': date_published,
                            'tags': tags,
                            'chunk_index': i,
                            'total_chunks': len(chunks),
                            'language': language,
                            'source': 'exploitdb'
                        }]
                    )
                
                imported_count += 1
                
            except Exception as e:
                error_count += 1
                print(f"\n‚ö†Ô∏è  Error processing paper {paper_id}: {str(e)}")
        
        print("\n" + "=" * 60)
        print("‚úÖ Import completed!")
        print(f"   ‚úì Imported: {imported_count} papers")
        print(f"   ‚úó Errors: {error_count} papers")
        print(f"   üìä Total documents in DB: {self.collection.count()}")
        print("=" * 60)
    
    def search_papers(self, query, n_results=5):
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ papers ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            n_results: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        """
        print(f"\nüîç Searching for: '{query}'")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö query
        query_embedding = self.embedding_model.encode(query).tolist()
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results
        )
        
        print(f"\nüìã Found {len(results['documents'][0])} results:\n")
        
        for i, (doc, metadata) in enumerate(zip(results['documents'][0], results['metadatas'][0])):
            print(f"{i+1}. {metadata.get('title', 'Untitled')}")
            print(f"   Author: {metadata.get('author', 'Unknown')}")
            print(f"   Date: {metadata.get('date', 'Unknown')}")
            print(f"   Preview: {doc[:200]}...")
            print()
        
        return results
    
    def get_stats(self):
        """‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á database"""
        count = self.collection.count()
        
        print("\nüìä Database Statistics:")
        print(f"   Total chunks: {count}")
        
        # ‡∏î‡∏∂‡∏á metadata ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏±‡∏ö papers
        if count > 0:
            all_data = self.collection.get()
            unique_papers = set()
            
            for metadata in all_data['metadatas']:
                paper_id = metadata.get('paper_id')
                if paper_id:
                    unique_papers.add(paper_id)
            
            print(f"   Unique papers: {len(unique_papers)}")
        
        print()


def main():
    """Main function"""
    print("=" * 60)
    print("  Exploit-DB Papers Importer")
    print("  Import security papers into RAG system")
    print("=" * 60)
    
    # ‡∏™‡∏£‡πâ‡∏≤‡∏á importer
    importer = ExploitDBImporter()
    
    # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô
    importer.get_stats()
    
    # ‡∏ñ‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
    print("\nOptions:")
    print("1. Import papers (English only)")
    print("2. Import papers (limit 100 for testing)")
    print("3. Search papers")
    print("4. Show statistics")
    print("5. Exit")
    
    choice = input("\nSelect option (1-5): ").strip()
    
    if choice == '1':
        print("\n‚ö†Ô∏è  This will import ALL English papers (~1000+ papers)")
        confirm = input("Continue? (yes/no): ").strip().lower()
        if confirm == 'yes':
            importer.import_papers(language='english')
    
    elif choice == '2':
        print("\nüìù Importing first 100 papers for testing...")
        importer.import_papers(language='english', limit=100)
    
    elif choice == '3':
        query = input("\nEnter search query: ").strip()
        if query:
            importer.search_papers(query)
    
    elif choice == '4':
        importer.get_stats()
    
    elif choice == '5':
        print("\nüëã Goodbye!")
        return
    
    else:
        print("\n‚ùå Invalid option")


if __name__ == "__main__":
    main()
