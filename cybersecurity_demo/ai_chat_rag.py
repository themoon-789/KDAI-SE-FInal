"""
AI Chat Module with Full RAG Implementation
‡πÉ‡∏ä‡πâ OpenRouter API + Vector Store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏à‡∏£‡∏¥‡∏á
"""

import os
import requests
from typing import Dict, Optional
from dotenv import load_dotenv

# ‡πÇ‡∏´‡∏•‡∏î environment variables
load_dotenv()

class AIChatRAG:
    def __init__(self, vector_store=None):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á AI Chat ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö RAG
        
        Args:
            vector_store: VectorStore instance ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        """
        self.api_key = os.getenv('OPENROUTER_API_KEY')
        self.model = os.getenv('OPENROUTER_MODEL', 'meta-llama/llama-3.2-3b-instruct:free')
        self.api_url = 'https://openrouter.ai/api/v1/chat/completions'
        self.vector_store = vector_store
        
        if not self.api_key:
            raise ValueError("OPENROUTER_API_KEY not found in environment variables")
    
    def chat_with_rag(self, user_message: str, use_rag: bool = True) -> Dict:
        """
        ‡∏ñ‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏±‡∏ö AI ‡∏û‡∏£‡πâ‡∏≠‡∏° RAG
        
        Args:
            user_message: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
            use_rag: ‡πÉ‡∏ä‡πâ RAG ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
            
        Returns:
            Dict ‡∏ó‡∏µ‡πà‡∏°‡∏µ response, context, sources
        """
        context = ""
        sources = []
        
        # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡∏¥‡∏î RAG ‡πÅ‡∏•‡∏∞‡∏°‡∏µ Vector Store
        if use_rag and self.vector_store:
            print(f"üîç Searching vector store for: {user_message}")
            
            # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
            search_results = self.vector_store.search(user_message, n_results=3)
            
            if search_results['results']:
                print(f"‚úÖ Found {len(search_results['results'])} relevant chunks")
                
                # ‡∏™‡∏£‡πâ‡∏≤‡∏á context ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
                context_parts = []
                for i, result in enumerate(search_results['results'], 1):
                    filename = result['metadata'].get('filename', 'Unknown')
                    text = result['text']
                    distance = result.get('distance', 0)
                    
                    context_parts.append(f"[Source {i}: {filename}]\n{text}")
                    sources.append({
                        'filename': filename,
                        'text_preview': text[:200] + '...' if len(text) > 200 else text,
                        'relevance_score': 1 - distance if distance else 1.0
                    })
                
                context = "\n\n".join(context_parts)
                print(f"üìö Context length: {len(context)} characters")
            else:
                print("‚ö†Ô∏è  No relevant documents found")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á system prompt
        if context:
            system_prompt = f"""You are a cybersecurity expert assistant with access to a knowledge base.

Use the following information from the knowledge base to answer the user's question:

{context}

Instructions:
- Answer based on the provided information
- If the information is not in the knowledge base, say so and provide general guidance
- Be concise and accurate
- Cite sources when possible"""
        else:
            system_prompt = """You are a cybersecurity expert assistant.
Provide helpful, accurate, and concise answers about cybersecurity topics."""
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á messages
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message}
        ]
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å LLM API
        try:
            print(f"ü§ñ Calling LLM: {self.model}")
            response = requests.post(
                self.api_url,
                headers={
                    "Authorization": f"Bearer {self.api_key}",
                    "Content-Type": "application/json",
                    "HTTP-Referer": "http://localhost:5001",
                    "X-Title": "Cybersecurity Agent RAG"
                },
                json={
                    "model": self.model,
                    "messages": messages,
                    "temperature": 0.7,
                    "max_tokens": 800
                },
                timeout=30
            )
            
            response.raise_for_status()
            result = response.json()
            
            # ‡∏î‡∏∂‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
            if 'choices' in result and len(result['choices']) > 0:
                ai_response = result['choices'][0]['message']['content']
                print(f"‚úÖ Got response: {len(ai_response)} characters")
                
                return {
                    'success': True,
                    'response': ai_response,
                    'context_used': bool(context),
                    'sources': sources,
                    'model': self.model
                }
            else:
                return {
                    'success': False,
                    'error': 'No response from LLM',
                    'context_used': False,
                    'sources': []
                }
                
        except requests.exceptions.Timeout:
            return {
                'success': False,
                'error': 'Request timeout. Please try again.',
                'context_used': bool(context),
                'sources': sources
            }
        except requests.exceptions.RequestException as e:
            error_msg = str(e)
            if '429' in error_msg:
                error_msg = 'Rate limit exceeded. Please wait a moment and try again.'
            return {
                'success': False,
                'error': error_msg,
                'context_used': bool(context),
                'sources': sources
            }
        except Exception as e:
            return {
                'success': False,
                'error': f'Unexpected error: {str(e)}',
                'context_used': False,
                'sources': []
            }
    
    def analyze_threat_with_context(self, threat_data: Dict) -> Dict:
        """
        ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏±‡∏¢‡∏Ñ‡∏∏‡∏Å‡∏Ñ‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢ AI + ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Knowledge Base
        
        Args:
            threat_data: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏±‡∏¢‡∏Ñ‡∏∏‡∏Å‡∏Ñ‡∏≤‡∏°
            
        Returns:
            ‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏à‡∏≤‡∏Å AI
        """
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        threat_type = threat_data.get('type', 'Unknown')
        query = f"How to handle {threat_type} attack? Security recommendations"
        
        # ‡πÉ‡∏ä‡πâ RAG ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        result = self.chat_with_rag(
            f"Analyze this security threat and provide recommendations:\n{threat_data}",
            use_rag=True
        )
        
        return result
    
    def get_model_info(self) -> Dict:
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏• LLM ‡πÅ‡∏•‡∏∞ Vector Store
        
        Returns:
            ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏°‡πÄ‡∏î‡∏•
        """
        info = {
            "model": self.model,
            "provider": "OpenRouter",
            "type": "Free LLM with RAG",
            "status": "active" if self.api_key else "inactive",
            "rag_enabled": self.vector_store is not None
        }
        
        if self.vector_store:
            stats = self.vector_store.get_stats()
            info.update({
                "vector_store": "ChromaDB",
                "total_documents": stats['total_documents'],
                "total_chunks": stats['total_chunks'],
                "embedding_model": stats['embedding_model']
            })
        
        return info
