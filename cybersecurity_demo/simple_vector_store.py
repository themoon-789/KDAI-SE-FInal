"""
Simple Vector Store - ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ ChromaDB
‡πÉ‡∏ä‡πâ in-memory storage ‡πÅ‡∏•‡∏∞ cosine similarity
"""

import os
import PyPDF2
import docx
import hashlib
import json
from datetime import datetime
from typing import List, Dict
import numpy as np

class SimpleVectorStore:
    def __init__(self, persist_file="./vector_data.json"):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Simple Vector Store
        
        Args:
            persist_file: ‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        """
        self.persist_file = persist_file
        self.documents = []  # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        self.load_data()
        print("‚úÖ Simple Vector Store initialized")
    
    def load_data(self):
        """‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå"""
        if os.path.exists(self.persist_file):
            try:
                with open(self.persist_file, 'r', encoding='utf-8') as f:
                    self.documents = json.load(f)
                print(f"‚úÖ Loaded {len(self.documents)} documents")
            except:
                self.documents = []
        else:
            self.documents = []
    
    def save_data(self):
        """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå"""
        with open(self.persist_file, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
    
    def extract_text_from_file(self, filepath: str) -> str:
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå"""
        ext = os.path.splitext(filepath)[1].lower()
        
        try:
            if ext == '.pdf':
                return self._extract_from_pdf(filepath)
            elif ext == '.docx':
                return self._extract_from_docx(filepath)
            elif ext == '.txt':
                return self._extract_from_txt(filepath)
            else:
                return f"Unsupported file type: {ext}"
        except Exception as e:
            return f"Error extracting text: {str(e)}"
    
    def _extract_from_pdf(self, filepath: str) -> str:
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF"""
        text = ""
        with open(filepath, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        return text.strip()
    
    def _extract_from_docx(self, filepath: str) -> str:
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å DOCX"""
        doc = docx.Document(filepath)
        text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
        return text.strip()
    
    def _extract_from_txt(self, filepath: str) -> str:
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å TXT"""
        with open(filepath, 'r', encoding='utf-8') as file:
            return file.read().strip()
    
    def chunk_text(self, text: str, chunk_size: int = 500) -> List[str]:
        """‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô chunks"""
        chunks = []
        words = text.split()
        
        current_chunk = []
        current_length = 0
        
        for word in words:
            current_chunk.append(word)
            current_length += len(word) + 1
            
            if current_length >= chunk_size:
                chunks.append(' '.join(current_chunk))
                current_chunk = []
                current_length = 0
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def simple_embedding(self, text: str) -> List[float]:
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á simple embedding (TF-IDF style)
        ‡πÉ‡∏ô‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏£‡∏¥‡∏á‡∏Ñ‡∏ß‡∏£‡πÉ‡∏ä‡πâ sentence-transformers
        """
        # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å
        text = text.lower()
        
        # ‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏î‡πâ‡∏≤‡∏ô cybersecurity
        keywords = [
            'attack', 'threat', 'vulnerability', 'malware', 'ransomware',
            'phishing', 'firewall', 'encryption', 'security', 'breach',
            'ddos', 'sql injection', 'xss', 'authentication', 'authorization',
            'password', 'network', 'intrusion', 'detection', 'prevention'
        ]
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á vector ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç
        vector = []
        for keyword in keywords:
            count = text.count(keyword)
            vector.append(count)
        
        # Normalize
        total = sum(vector) or 1
        vector = [v / total for v in vector]
        
        return vector
    
    def cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì cosine similarity"""
        if not vec1 or not vec2:
            return 0.0
        
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = sum(a * a for a in vec1) ** 0.5
        magnitude2 = sum(b * b for b in vec2) ** 0.5
        
        if magnitude1 == 0 or magnitude2 == 0:
            return 0.0
        
        return dot_product / (magnitude1 * magnitude2)
    
    def add_document(self, filepath: str, metadata: Dict = None) -> Dict:
        """‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤ Vector Store"""
        print(f"üìÑ Processing {filepath}...")
        
        # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
        text = self.extract_text_from_file(filepath)
        
        if not text or len(text) < 10:
            return {
                'success': False,
                'error': 'No text extracted from file'
            }
        
        # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô chunks
        chunks = self.chunk_text(text)
        print(f"‚úÇÔ∏è  Created {len(chunks)} chunks")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings
        filename = os.path.basename(filepath)
        file_hash = hashlib.md5(filepath.encode()).hexdigest()[:8]
        
        for i, chunk in enumerate(chunks):
            embedding = self.simple_embedding(chunk)
            
            doc = {
                'id': f"{file_hash}_{i}",
                'filename': filename,
                'text': chunk,
                'embedding': embedding,
                'chunk_index': i,
                'total_chunks': len(chunks),
                'upload_time': metadata.get('upload_time', datetime.now().isoformat()) if metadata else datetime.now().isoformat()
            }
            
            self.documents.append(doc)
        
        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
        self.save_data()
        
        print(f"‚úÖ Added {len(chunks)} chunks")
        
        return {
            'success': True,
            'filename': filename,
            'chunks': len(chunks),
            'total_chars': len(text)
        }
    
    def search(self, query: str, n_results: int = 5) -> Dict:
        """‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á"""
        if not self.documents:
            return {
                'query': query,
                'results': []
            }
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö query
        query_embedding = self.simple_embedding(query)
        
        # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì similarity ‡∏Å‡∏±‡∏ö‡∏ó‡∏∏‡∏Å document
        results = []
        for doc in self.documents:
            similarity = self.cosine_similarity(query_embedding, doc['embedding'])
            results.append({
                'text': doc['text'],
                'metadata': {
                    'filename': doc['filename'],
                    'chunk_index': doc['chunk_index'],
                    'total_chunks': doc['total_chunks']
                },
                'distance': 1 - similarity,  # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô distance
                'similarity': similarity
            })
        
        # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏° similarity (‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Å‡πà‡∏≠‡∏ô)
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        # ‡πÄ‡∏≠‡∏≤‡πÅ‡∏Ñ‡πà top n
        top_results = results[:n_results]
        
        return {
            'query': query,
            'results': top_results
        }
    
    def get_context_for_query(self, query: str, max_chunks: int = 3) -> str:
        """‡∏î‡∏∂‡∏á context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAG"""
        search_results = self.search(query, n_results=max_chunks)
        
        if not search_results['results']:
            return ""
        
        context_parts = []
        for i, result in enumerate(search_results['results'], 1):
            filename = result['metadata'].get('filename', 'Unknown')
            text = result['text']
            context_parts.append(f"[Document: {filename}]\n{text}")
        
        return "\n\n".join(context_parts)
    
    def get_stats(self) -> Dict:
        """‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥"""
        unique_files = set(doc['filename'] for doc in self.documents)
        
        return {
            'total_chunks': len(self.documents),
            'total_documents': len(unique_files),
            'collection_name': 'simple_vector_store',
            'embedding_model': 'simple-tf-idf'
        }
    
    def delete_document(self, filename: str) -> Dict:
        """‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£"""
        initial_count = len(self.documents)
        self.documents = [doc for doc in self.documents if doc['filename'] != filename]
        deleted_count = initial_count - len(self.documents)
        
        if deleted_count > 0:
            self.save_data()
            return {
                'success': True,
                'deleted_chunks': deleted_count
            }
        else:
            return {
                'success': False,
                'error': 'Document not found'
            }
    
    def reset(self):
        """‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î"""
        self.documents = []
        self.save_data()
        print("‚úÖ Vector store reset")
