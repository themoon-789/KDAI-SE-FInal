"""
Vector Store Module - Full RAG Implementation
‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Vector Embeddings ‡πÅ‡∏•‡∏∞ Semantic Search
"""

import os
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Optional
import PyPDF2
import docx
import hashlib
from datetime import datetime

class VectorStore:
    def __init__(self, persist_directory="./chroma_db"):
        """
        ‡∏™‡∏£‡πâ‡∏≤‡∏á Vector Store ‡∏î‡πâ‡∏ß‡∏¢ ChromaDB
        
        Args:
            persist_directory: ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö Vector Database
        """
        self.persist_directory = persist_directory
        os.makedirs(persist_directory, exist_ok=True)
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á ChromaDB client
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á collection ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        try:
            self.collection = self.client.get_collection("cybersecurity_docs")
            print(f"‚úÖ Loaded existing collection with {self.collection.count()} documents")
        except:
            self.collection = self.client.create_collection(
                name="cybersecurity_docs",
                metadata={"description": "Cybersecurity knowledge base"}
            )
            print("‚úÖ Created new collection")
        
        # ‡πÇ‡∏´‡∏•‡∏î Embedding Model (‡πÉ‡∏ä‡πâ model ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß)
        print("üì• Loading embedding model...")
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        print("‚úÖ Embedding model loaded")
    
    def extract_text_from_file(self, filepath: str) -> str:
        """
        ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå (PDF, DOCX, TXT)
        
        Args:
            filepath: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
            
        Returns:
            ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡πÅ‡∏¢‡∏Å‡πÑ‡∏î‡πâ
        """
        ext = os.path.splitext(filepath)[1].lower()
        
        try:
            if ext == '.pdf':
                return self._extract_from_pdf(filepath)
            elif ext == '.docx':
                return self._extract_from_docx(filepath)
            elif ext == '.txt':
                return self._extract_from_txt(filepath)
            else:
                return f"Unsupported file type: {ext}"
        except Exception as e:
            return f"Error extracting text: {str(e)}"
    
    def _extract_from_pdf(self, filepath: str) -> str:
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF"""
        text = ""
        with open(filepath, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
        return text.strip()
    
    def _extract_from_docx(self, filepath: str) -> str:
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å DOCX"""
        doc = docx.Document(filepath)
        text = "\n".join([paragraph.text for paragraph in doc.paragraphs])
        return text.strip()
    
    def _extract_from_txt(self, filepath: str) -> str:
        """‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å TXT"""
        with open(filepath, 'r', encoding='utf-8') as file:
            return file.read().strip()
    
    def chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
        """
        ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô chunks ‡πÄ‡∏û‡∏∑‡πà‡∏≠ embedding
        
        Args:
            text: ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÅ‡∏ö‡πà‡∏á
            chunk_size: ‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞ chunk (characters)
            overlap: ‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡∏ã‡πâ‡∏≠‡∏ô‡∏ó‡∏±‡∏ö‡∏Å‡∏±‡∏ô (characters)
            
        Returns:
            List ‡∏Ç‡∏≠‡∏á text chunks
        """
        chunks = []
        start = 0
        text_length = len(text)
        
        while start < text_length:
            end = start + chunk_size
            chunk = text[start:end]
            
            # ‡∏´‡∏≤‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏µ (‡∏ó‡πâ‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ)
            if end < text_length:
                last_period = chunk.rfind('.')
                last_newline = chunk.rfind('\n')
                cut_point = max(last_period, last_newline)
                
                if cut_point > chunk_size * 0.5:  # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏à‡∏∏‡∏î‡∏ï‡∏±‡∏î‡∏ó‡∏µ‡πà‡∏î‡∏µ
                    chunk = chunk[:cut_point + 1]
                    end = start + cut_point + 1
            
            if chunk.strip():
                chunks.append(chunk.strip())
            
            start = end - overlap
        
        return chunks
    
    def add_document(self, filepath: str, metadata: Dict = None) -> Dict:
        """
        ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤ Vector Store
        
        Args:
            filepath: path ‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå
            metadata: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° (filename, upload_time, etc.)
            
        Returns:
            ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£
        """
        # ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå
        print(f"üìÑ Extracting text from {filepath}...")
        text = self.extract_text_from_file(filepath)
        
        if not text or len(text) < 10:
            return {
                'success': False,
                'error': 'No text extracted from file'
            }
        
        # ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô chunks
        print(f"‚úÇÔ∏è  Chunking text...")
        chunks = self.chunk_text(text)
        print(f"‚úÖ Created {len(chunks)} chunks")
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embeddings
        print(f"üßÆ Creating embeddings...")
        embeddings = self.embedding_model.encode(chunks).tolist()
        
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á unique IDs
        file_hash = hashlib.md5(filepath.encode()).hexdigest()[:8]
        ids = [f"{file_hash}_{i}" for i in range(len(chunks))]
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° metadata
        if metadata is None:
            metadata = {}
        
        filename = os.path.basename(filepath)
        metadatas = [
            {
                'filename': filename,
                'chunk_index': i,
                'total_chunks': len(chunks),
                'upload_time': metadata.get('upload_time', datetime.now().isoformat()),
                'file_path': filepath
            }
            for i in range(len(chunks))
        ]
        
        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏Ç‡πâ‡∏≤ ChromaDB
        print(f"üíæ Adding to vector store...")
        self.collection.add(
            ids=ids,
            embeddings=embeddings,
            documents=chunks,
            metadatas=metadatas
        )
        
        print(f"‚úÖ Successfully added {len(chunks)} chunks to vector store")
        
        return {
            'success': True,
            'filename': filename,
            'chunks': len(chunks),
            'total_chars': len(text),
            'file_hash': file_hash
        }
    
    def search(self, query: str, n_results: int = 5) -> Dict:
        """
        ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏î‡πâ‡∏ß‡∏¢ Semantic Search
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
            n_results: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£
            
        Returns:
            ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤
        """
        # ‡∏™‡∏£‡πâ‡∏≤‡∏á embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö query
        query_embedding = self.embedding_model.encode([query]).tolist()
        
        # ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÉ‡∏ô ChromaDB
        results = self.collection.query(
            query_embeddings=query_embedding,
            n_results=n_results
        )
        
        # ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
        formatted_results = {
            'query': query,
            'results': []
        }
        
        if results['documents'] and len(results['documents'][0]) > 0:
            for i in range(len(results['documents'][0])):
                formatted_results['results'].append({
                    'text': results['documents'][0][i],
                    'metadata': results['metadatas'][0][i],
                    'distance': results['distances'][0][i] if 'distances' in results else None
                })
        
        return formatted_results
    
    def get_context_for_query(self, query: str, max_chunks: int = 3) -> str:
        """
        ‡∏î‡∏∂‡∏á context ‡∏à‡∏≤‡∏Å Vector Store ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (RAG)
        
        Args:
            query: ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°
            max_chunks: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô chunks ‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ
            
        Returns:
            Context text ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM
        """
        search_results = self.search(query, n_results=max_chunks)
        
        if not search_results['results']:
            return ""
        
        # ‡∏£‡∏ß‡∏° context ‡∏à‡∏≤‡∏Å chunks ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        context_parts = []
        for i, result in enumerate(search_results['results'], 1):
            filename = result['metadata'].get('filename', 'Unknown')
            text = result['text']
            context_parts.append(f"[Document: {filename}]\n{text}")
        
        return "\n\n".join(context_parts)
    
    def get_stats(self) -> Dict:
        """
        ‡∏î‡∏∂‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Ç‡∏≠‡∏á Vector Store
        
        Returns:
            ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏ï‡πà‡∏≤‡∏á‡πÜ
        """
        count = self.collection.count()
        
        # ‡∏ô‡∏±‡∏ö‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥
        if count > 0:
            all_metadata = self.collection.get()['metadatas']
            unique_files = set(m.get('filename', '') for m in all_metadata)
            num_files = len(unique_files)
        else:
            num_files = 0
        
        return {
            'total_chunks': count,
            'total_documents': num_files,
            'collection_name': self.collection.name,
            'embedding_model': 'all-MiniLM-L6-v2'
        }
    
    def delete_document(self, filename: str) -> Dict:
        """
        ‡∏•‡∏ö‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Vector Store
        
        Args:
            filename: ‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏•‡∏ö
            
        Returns:
            ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏•‡∏ö
        """
        # ‡∏´‡∏≤ IDs ‡∏Ç‡∏≠‡∏á chunks ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á
        all_data = self.collection.get()
        ids_to_delete = []
        
        for i, metadata in enumerate(all_data['metadatas']):
            if metadata.get('filename') == filename:
                ids_to_delete.append(all_data['ids'][i])
        
        if ids_to_delete:
            self.collection.delete(ids=ids_to_delete)
            return {
                'success': True,
                'deleted_chunks': len(ids_to_delete)
            }
        else:
            return {
                'success': False,
                'error': 'Document not found'
            }
    
    def reset(self):
        """‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô Vector Store"""
        self.client.delete_collection("cybersecurity_docs")
        self.collection = self.client.create_collection(
            name="cybersecurity_docs",
            metadata={"description": "Cybersecurity knowledge base"}
        )
        print("‚úÖ Vector store reset")
